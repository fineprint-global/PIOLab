{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge floweaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install floweaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge ipysankeywidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge nodejs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbextension enable --py --sys-prefix ipysankeywidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbextension enable --py --sys-prefix widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbextension enable --py widgetsnbextension --sys-prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example with code: \n",
    "https://towardsdatascience.com/creating-beautiful-sankey-diagrams-with-floweaver-dc1f02fe76bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from floweaver import *\n",
    "from  ipysankeywidget import *\n",
    "from os import *\n",
    "import shutil\n",
    "from os.path import isfile, join\n",
    "import itertools\n",
    "# from Sankey_code import create_Sankey\n",
    "\n",
    "#load region and country names \n",
    "regs = pd.read_excel('Countries_and_Regions.xlsx', sheet_name = 'regions', index_col=0,engine='openpyxl')['Region']\n",
    "counts = pd.read_excel('Countries_and_Regions.xlsx', sheet_name = 'names', index_col=0,engine='openpyxl')['Name']\n",
    "\n",
    "#define path and list all sankey data files\n",
    "m=getcwd()[0:(len(os.getcwd())-6)]+str('output\\\\666\\\\')\n",
    "\n",
    "\n",
    "####################create itearative sankey images and savethem in a folder\n",
    "#deleting and creating new folder with sankey images\n",
    "# if path.exists('Sankey_Images'):\n",
    "#     shutil.rmtree('Sankey_Images', ignore_errors=False, onerror=None)\n",
    "# mkdir('Sankey_Images')\n",
    "\n",
    "year = 2008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the font size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".sankey .node {\n",
       "    font-size: 28px;\n",
       "    text-align: center;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".sankey .node {\n",
    "    font-size: 28px;\n",
    "    text-align: center;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nr\n",
       "1                  Austria\n",
       "2                  Belgium\n",
       "3           Czech Republic\n",
       "4                  Germany\n",
       "5                    Spain\n",
       "6                  Finland\n",
       "7                   France\n",
       "8                   Greece\n",
       "9                    Italy\n",
       "10             Netherlands\n",
       "11                  Poland\n",
       "12                  Sweden\n",
       "13                Slovakia\n",
       "14          United Kingdom\n",
       "15           United States\n",
       "16                   Japan\n",
       "17                   China\n",
       "18                  Canada\n",
       "19             South Korea\n",
       "20                  Brazil\n",
       "21                   India\n",
       "22                  Mexico\n",
       "23                  Russia\n",
       "24               Australia\n",
       "25                  Taiwan\n",
       "26               Indonesia\n",
       "27    RoW Asia and Pacific\n",
       "28             RoW America\n",
       "29              RoW Europe\n",
       "30                RoW EU27\n",
       "31              RoW Africa\n",
       "32         RoW Middle East\n",
       "Name: Name, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts\n",
    "#regs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code creates the Sankeys for the Countries and Regions specified in the counts and regs arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China\n",
      "[[['start1'], [], []], [['layer1'], ['start2'], ['start3']], [['end1'], ['layer2', 'layer7'], []], [['end2'], ['layer3'], ['w1']], [['end3'], ['layer4'], []], [[], ['layer5'], []], [[], ['layer6'], ['w7']], [[], ['layer8', 'layer9', 'layer10', 'layer11', 'layer12'], []]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d631aabfa514aba997a3079788a49de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SankeyWidget(groups=[{'id': '__w1_layer7_2', 'type': 'group', 'title': '', 'nodes': ['__w1_layer7_2^*']}, {'id…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for region in regs:\n",
    "region=regs[4]#regions from 1-6\n",
    "# region = counts[26]#counts from 1-32\n",
    "\n",
    "#Read PIOT for the specific country\n",
    "dg = pd.read_excel(str(m)+str(year)+'_Data_for_'+str(region)+'_Sankey.xlsx', sheet_name = 'agg', index_col=0,engine='openpyxl')\n",
    "dg = dg.round(2)\n",
    "\n",
    "\n",
    "print(region)\n",
    "\n",
    "\n",
    "\n",
    "#create empy dataframe, which will be a form of a PIVOT table\n",
    "d0 = pd.DataFrame()\n",
    "\n",
    "#run through rows of the Piot\n",
    "for i in range(0,dg.shape[0]):\n",
    "    #create empty data frame for temporary saving results\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    #save the values of the row\n",
    "    val_orig = dg.iloc[i,].values\n",
    "    #save indices where values are not 0 and not NA\n",
    "    ind = [all(tup) for tup in zip(val_orig!=0, np.isnan(val_orig)==False)]\n",
    "    \n",
    "    #save results for value, source & target\n",
    "    val = val_orig[ind]\n",
    "    source = [*[dg.index[i]] * len(val)]\n",
    "    target = dg.columns[ind]\n",
    "\n",
    "    #save results in the relevant columns\n",
    "    df['source']=source\n",
    "    df['target']=target\n",
    "    df['value']=val\n",
    "\n",
    "    #append the collected results to the main data frame\n",
    "    d0=d0.append(df, ignore_index =True)\n",
    "\n",
    "#add the column 'type' and set all values in it to 'A'    \n",
    "d0['type']='A'\n",
    "\n",
    "#change types according to their teal type of flow\n",
    "d0.at[d0[d0['source'].str.contains(\"Domestic\")].index,'type'] = 'D'\n",
    "d0.at[d0[d0['source'].str.contains(\"Foreign\")].index,'type'] = 'F'\n",
    "d0.at[d0[d0['source'].str.contains(\"Bound\")].index,'type'] = 'BI'\n",
    "d0.at[d0[d0['target'].str.contains(\"Bound\")].index,'type'] = 'BO'\n",
    "d0.at[d0[d0['source'].str.contains(\"ROW\")].index,'type'] = 'ROW'\n",
    "\n",
    "#change source names, which means to use the column names, because they do not include the Domestic or Foreign String\n",
    "for i in dg.columns.values:\n",
    "    d0.at[d0[d0['source'].str.contains(str(i))].index,'source'] = str(i)\n",
    "\n",
    "#we change the name of the main data frame to df    \n",
    "df=d0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Now, we have to change some names, e.g specify the BO and BI which are now only saved as 'Boundary Outputs' or 'Inputs'\n",
    "#in the data frame. Further we have to insert also the string '\\n' which will be a Zeilenumbruch in the Plot, if the name is\n",
    "#too long!\n",
    "\n",
    "#define name-pairs for the boundary outputs (BO), could also be read in from a csv, but for now \n",
    "#they can be changed here in the code\n",
    "BO=[['Mining', 'BO Gangue'],\n",
    "    ['Reduction','BO Blast furnance\\ngas & slag'],\n",
    "    ['Steelmaking','BO Steelmaking Slag']]\n",
    "\n",
    "#do the same for the boundary inputs (BI)\n",
    "BI=[['BI Crude Ore', 'Mining'],\n",
    "    ['BI Coke/air/flux','Reduction'],\n",
    "    ['BI Eol-Scrap','Scrap preparation']]\n",
    "\n",
    "#define also new(=shorter) names pairs Final Outputs strings\n",
    "FU=[['Construction','Construction'],\n",
    "    ['Manufacture of machinery and equipment n.e.c.','Machinery and \\nequipment n.e.c.'],\n",
    "   ['Manufacture of motor vehicles, trailers and semi-trailers','Motor vehicles, trailers \\nand semi-trailers'],\n",
    "    ['Manufacturing n.e.c.','Manufacturing n.e.c.'],\n",
    "   ['Manufacture of other transport equipment','Other transport \\nequipment']\n",
    "    ]\n",
    "\n",
    "#rename Boundary Output & Inputs names for all relevant rows\n",
    "for i in range(0,len(BO)):\n",
    "    df.loc[(df['source']==BO[i][0]) & (df['target']==dg.columns[dg.shape[1]-1]),'target']=BO[i][1]\n",
    "\n",
    "for i in range(0,len(BI)):\n",
    "    df.loc[(df['target']==BI[i][1]) & (df['source']==dg.index[dg.shape[0]-1]),'source']=BI[i][0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#here we save the detailed final use names\n",
    "dv_names = pd.read_excel(str(m)+str(year)+'_Data_for_'+str(region)+'_Sankey.xlsx', sheet_name = 'disagg_names', index_col=0,engine='openpyxl',header=None)\n",
    "dv_names=dv_names.reset_index()\n",
    "\n",
    "#...and the detailed final use values with their names as a data frames\n",
    "dv = pd.read_excel(str(m)+str(year)+'_Data_for_'+str(region)+'_Sankey.xlsx', sheet_name = 'disagg', index_col=0,engine='openpyxl')\n",
    "dv = dv.round(2)\n",
    "dv=dv.reset_index()\n",
    "dv.columns=['target','value']\n",
    "dv['source']=dg.columns[-4]\n",
    "dv['type']='A'\n",
    "\n",
    "\n",
    "#...and again change types in the type column\n",
    "dv.at[dv[dv['target'].str.contains(\"Domestic\")].index,'type'] = 'D'\n",
    "dv.at[dv[dv['target'].str.contains(\"Foreign\")].index,'type'] = 'F'\n",
    "dv.at[dv[dv['target'].str.contains(\"ROW\")].index,'type'] = 'ROW'\n",
    "dv['target']=np.concatenate(np.tile(dv_names[0].values, (3, 1)))\n",
    "dv=dv[dv['value']!=0]\n",
    "\n",
    "#here we rename the main data frame for the final use details in db\n",
    "db=dv\n",
    "\n",
    "#and we append the detailes final use data frame to the main data frame\n",
    "df=df.append(db)\n",
    "#drop Final Use rows because we now have the detailed data appended\n",
    "df=df[df['target']!='Final Use']\n",
    "\n",
    "# now we change the detailed final use names to the shorter names we specified above\n",
    "for i in range(0,len(FU)):\n",
    "    df.loc[df['target']==FU[i][0],'target']=FU[i][1]\n",
    "\n",
    "#for indexing we need to reset the index (needs to be sometimes if we append data frames)\n",
    "df=df.reset_index()\n",
    "df=df.drop('index',axis=1)#in this process a new column with the old index is created\n",
    "#but we drop this because it is useless\n",
    "df=df.round(2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create dataframes for the sums we want to display in the final plot\n",
    "sums=[]\n",
    "for nam in np.unique(np.append(df['source'].values,df['target'].values)).tolist():\n",
    "    sums.append(df[df['source']==str(nam)]['value'].sum())\n",
    "\n",
    "#create he data frame with the aggregates for the flows which we have as sources (almost all)\n",
    "dx=pd.DataFrame()\n",
    "dx['flow']=np.unique(np.append(df['source'].values,df['target'].values)).tolist()\n",
    "dx['value']= sums\n",
    "dx=dx[dx['value']!=0]\n",
    "dx=dx.round(2)\n",
    "\n",
    "#then we do exactly the same for flows we only have as targets, e.g. Final Uses and BO´s\n",
    "sums1=[]\n",
    "for nam in np.unique(df['target'].values).tolist():\n",
    "    sums1.append(df[df['target']==str(nam)]['value'].sum())\n",
    "\n",
    "dy=pd.DataFrame()\n",
    "dy['flow']=np.unique(df['target'].values).tolist()\n",
    "dy['value']= sums1\n",
    "dy=dy[dy['value']!=0]\n",
    "dy=dy.round(2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#define lists of layer dictionaries, layer1='Mining', layer2='Reduction', ...\n",
    "#this lists are named layer,layer1 and layer2, which is a bit confusing because the first two lists contains the flow-nodes \n",
    "#of the plot which are also named layer1,2,3... whereas the list layer2 contains the nodes of the BOs which are named end1,2,3.\n",
    "#The nodes for the BIs are defined later on.\n",
    "\n",
    "#this dicts contains next to the name, which is saved in a list, also a title, which is displayed later in the plot. \n",
    "#And because we also want the Sum of the Flows displayed, we have this huge complex looking title.\n",
    "#But this title is basicly the name of the flow and then the sum of the values which are going out of this flow.\n",
    "#The whole thing looks complicated, becuase the indexing of lists in python is that ugly, especially if we have a \n",
    "#further list as index. But we need this strange index lists, because they read out automaticly the correct BOs and BIs,\n",
    "#and skip the BOs or BIs which are not in the data, to avoid any errors.\n",
    "\n",
    "#first do all column which are a source in the plot\n",
    "layers = {'layer'+str(i+1):ProcessGroup(list([dg.columns[i]]), \n",
    "            title=dg.columns[i]+'\\n'+str(dx[dx['flow']==dg.columns[i]]['value'].values[0])+'\\n'\n",
    "                            ) for i in range(0,len(dg.columns)-2)}\n",
    "\n",
    "#then we do the same for those which are only a target\n",
    "layers1 = {'layer'+str(i+len(dg.columns)-1):ProcessGroup(list([[FU[i][1] for i in range(len(FU))][i]]),\n",
    "            title=[FU[j][1] for j in range(len(FU))][i]+'\\n'+str(dy[dy['flow']==[FU[j][1] for j in range(len(FU))][i]]['value'].values[0])+'\\n'\n",
    "            ) for i in range(0,len(np.unique(db['target'].values).tolist()))}\n",
    "\n",
    "\n",
    "#and finally the BO nodes\n",
    "BO_ind=[[v[1] for v in BO].index([ e for e in [v[1] for v in BO] if e in dy['flow'].values ][z]) for z in range(len([ e for e in [v[1] for v in BO] if e in dy['flow'].values ]))]\n",
    "layers2 = {'end'+str(i+1):ProcessGroup(list([[v[1] for v in BO][i]]), title=[v[1] for v in BO][i]+'\\n'+str(dy[dy['flow']==[v[1] for v in BO][i]]['value'].values[0])+'\\n')\n",
    "          for i in BO_ind}\n",
    "\n",
    "\n",
    "#get columns/flows which have flows to the scrap node:\n",
    "scrap_flows = df[(df['target'].str.contains('Scrap')) & (df['type']!='BI')]['source'].values\n",
    "#get the name of the scrap layer, becuase we need that often\n",
    "scrap_layer = list(layers.keys())[np.where(dg.columns.str.contains('Scrap'))[0][0]]\n",
    "#get layer-endings(=the number at the end of the string'layer1') & full layer names which have flows to scrap node\n",
    "layer_endings=[list(layers.keys())[np.where(dg.columns==scrap_flows[i])[0][0]][-1] for i in range(len(scrap_flows))]\n",
    "layer_fullnames=[list(layers.keys())[np.where(dg.columns==scrap_flows[i])[0][0]] for i in range(len(scrap_flows))]\n",
    "\n",
    "\n",
    "##############Ordering: here we specify the order of the nodes, starting with some fixed positions\n",
    "#first col: 1st start layer\n",
    "#beginning with the BI curde Ore node\n",
    "a=[['start1'],[],[]]\n",
    "\n",
    "#second col: followed by Mining and the other start layers\n",
    "b=[['layer1'],['start2'],['start3']] \n",
    "\n",
    "#third col:first end layer the reduction layer and the scrap layer\n",
    "c=[['end1'],['layer2',scrap_layer],[]] \n",
    "\n",
    "#define empty ordering list which we fill with the predefined columns\n",
    "ordering=list()\n",
    "ordering=ordering+[a]+[b]+[c]\n",
    "\n",
    "#layers inbetween:\n",
    "#make empty list for waypoints:\n",
    "waypoints=list({'w1': Waypoint(direction='L', title='')}.items())+list({'ws5': Waypoint(direction='L', title='')}.items())+list({'ws6': Waypoint(direction='L', title='')}.items())\n",
    "\n",
    "\n",
    "#for i in all other layer endings except the scrap layer ending\n",
    "j=1\n",
    "for i in [x for x in [v[-1] for v in list(layers.keys())[2:]] if (x!=scrap_layer[-1])]:#&(x!=Final_use_layer[-1])]: \n",
    "    o = [[],['layer'+str(i)],[]]#if no further if conditions are fulfilled then this is the next ordering column\n",
    "    if j<len(BO):#if we have not all end(=BOs) nodes placed,...\n",
    "        j=j+1\n",
    "        o = [['end'+str(j)],['layer'+str(i)],[]]#...then we have to place them next to the current layer we are looking at\n",
    "        \n",
    "        #but if we are looking at the first layer after the scrap layer, we also place a waypoint w1 to have a better plot:\n",
    "        if i==[x for x in [v[-1] for v in list(layers.keys())[2:]] if (x!=scrap_layer[-1])][0]:\n",
    "            o = [['end'+str(j)],['layer'+str(i)],['w1']]\n",
    "\n",
    "        #and if we have a layer in the current ordering column, where the layer in the next ordering column has a flow\n",
    "        #back to the scrap layer, we have to set here another waypoint for a better plot:\n",
    "        if int(i)+1 in [int(numeric_string)+1 for numeric_string in layer_endings]:  \n",
    "            o = [['end'+str(j)],['layer'+str(i)],['w'+str(int(i)+1)]]\n",
    "        #we also save this waypoint in the waypoint list\n",
    "        waypoints=waypoints+list({'w'+str(int(i)+1): Waypoint(direction='L', title='')}.items())\n",
    "    \n",
    "    #...and if we have already all ends played, we only have to playe the waypoints if the layer in the next ordering\n",
    "    #column has a flow back to the scrap layer\n",
    "    else:\n",
    "        if int(i)+1 in [int(numeric_string)+1 for numeric_string in layer_endings]:  \n",
    "            o = [[],['layer'+str(i)],['w'+str(int(i)+1)]]\n",
    "        #we also save this waypoint in the waypoint list\n",
    "        waypoints=waypoints+list({'w'+str(int(i)+1): Waypoint(direction='L', title='')}.items())\n",
    "    #then we append the ordering column\n",
    "    ordering=ordering+[o]\n",
    "\n",
    "#last col: the 5 Final Uses detailed are specified\n",
    "d=[[],['layer'+str(i+len(dg.columns)-1) for i in range(0,len(np.unique(db['target'].values).tolist()))],[]]\n",
    "\n",
    "#and then add the last column\n",
    "ordering=ordering+[d]\n",
    "print(ordering)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################################   NODES: \n",
    "#now we define the BI nodes, start 1,2,3\n",
    "nodes = dict(list({'start'+str(i): ProcessGroup(list([[v[0] for v in BI][i-1]]), title=[v[0] for v in BI][i-1]) for i in [1,2,3]}.items())+\n",
    "     list({'end'+str(i+1): ProcessGroup(list([[v[1] for v in BO][i]]), title=[v[1] for v in BO][i]) for i in BO_ind}.items()))\n",
    "\n",
    "#now we save which of the 3 BOs is missing in the data and skip this node\n",
    "missing_list = list(set(['end1','end2', 'end3'])-set(list(layers2.keys())))\n",
    "missing_ends = []\n",
    "if missing_list!=[]:\n",
    "    missing_ends = list({'end'+i[3]:ProcessGroup(list('Missing')) for i in list(set(['end1','end2', 'end3'])-set(list(layers2.keys())))}.items())\n",
    "\n",
    "#here we add the nodes in the node list together\n",
    "nodes = dict(list(nodes.items()) + list(layers.items()) + list(layers1.items()) + list(layers2.items()) + waypoints + missing_ends)\n",
    "\n",
    "#####################################################   BUNDLES:\n",
    "#the bundles define which cnnections between the nodes should be displayed, and it is forbidden to include duplicates.\n",
    "#Also one can specify if waypoints are included in the flow and the direction of the flow.\n",
    "\n",
    "#define a empty list and get the boolean list of indexes in which ordering column we have an waypoint\n",
    "waypointsindex = list([])\n",
    "for y in [[x for x in a][2] for a in ordering]:\n",
    "    if len(y)>0:\n",
    "        if 'w' in y[0]:\n",
    "            waypointsindex=waypointsindex+[True]\n",
    "        else: \n",
    "            waypointsindex=waypointsindex+[False]\n",
    "    else:\n",
    "        waypointsindex=waypointsindex+[False]\n",
    "\n",
    "#save the layers which have a waypoint(except 'w1') in their column ...\n",
    "wp_layers = [[[x for x in a][1] for a in ordering][y] for y in (np.asarray(([x for x in [np.where(waypointsindex)]][0]))[0]).tolist()][1:]\n",
    "#...and the names of this waypoints\n",
    "wp_waypoints = [[[x for x in a][2] for a in ordering][y] for y in (np.asarray(([x for x in [np.where(waypointsindex)]][0]))[0]).tolist()][1:]\n",
    "\n",
    "wp_strings = list(['w1'])\n",
    "waypointstrings= list([])\n",
    "\n",
    "#if we have several waypoints the the flow should run through them in reversed order, from right to left: [w7,w6,w5,w1]\n",
    "for i in range(len(wp_layers)):\n",
    "    wp_strings=[wp_waypoints[i][0]]+wp_strings\n",
    "    waypointstrings=waypointstrings+[wp_strings]\n",
    "\n",
    "#now we add all teh relevant bundles to gether in a huge list:\n",
    "bundles = [Bundle('layer'+str(i), 'layer'+str(i+1)) for i in [v+1 for v in range(len(dg.columns)-4)]]+\\\n",
    "[Bundle(scrap_layer, 'layer'+str(i)) for i in [v+1 for v in range(len(dg.columns)-3)]]+\\\n",
    "list(itertools.chain.from_iterable([[Bundle('start'+str(j+1),'layer'+str(i)) for i in [v+1 for v in range(len(dg.columns)-1)]] for j in range(len(BI))]))+\\\n",
    "list(itertools.chain.from_iterable([[Bundle('layer'+str(i),'end'+str(j+1)) for i in [v+1 for v in range(len(dg.columns)-1)]] for j in BO_ind]))+\\\n",
    "[Bundle('layer'+str(len(dg.columns)-3),'layer'+str(len(dg.columns)-2+i)) for i in range(1,len(np.unique(db['target'].values).tolist())+1)]+\\\n",
    "[Bundle(wp_layers[i][0], scrap_layer, waypointstrings[i]) for i in range(len(wp_layers))]+\\\n",
    "[Bundle(source='layer4', target='layer3', waypoints=(), flow_selection=None, flow_partition=None, default_partition=None),\n",
    " Bundle(source='layer5', target='layer3', waypoints=(), flow_selection=None, flow_partition=None, default_partition=None)]\n",
    "\n",
    "\n",
    "#####################################################    PLOT:\n",
    "# add the partitions which determine the column partition\n",
    "partition_type = Dataset(df).partition('type')\n",
    "\n",
    "# add the color palette\n",
    "palette = {'D': '#481567',#'#FF00FF',#'#440154FF', #'#541352',\n",
    "           'F': '#FDE725', #'#FFD700',\n",
    "          'BI': '#F88379',#'#10a53d',\n",
    "          'BO': '#F5F5F5',\n",
    "          'ROW': '#00FF00'}\n",
    "\n",
    "\n",
    "# create the sankey diagram\n",
    "sdd = SankeyDefinition(nodes, bundles, ordering, \n",
    "                       flow_partition=partition_type)\n",
    "\n",
    "# display & save the sankey diagram\n",
    "weave(sdd, df, palette=palette\n",
    "     ).to_widget(width=2680, height=720, margins=dict(left=220, right=320),align_link_types=False).auto_save_png(str(m)+'Sankey_Images\\\\'+str(year)+'_'+str(region)+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here is almost the same Code which creates the sankey for the World and World-without-China Sankeys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World-without-China\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e92f57d2a94990b38c9aeb9e4857d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SankeyWidget(groups=[{'id': '__w1_layer7_2', 'type': 'group', 'title': '', 'nodes': ['__w1_layer7_2^*']}, {'id…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for region in regs:\n",
    "\n",
    "region='World'\n",
    "region='World-without-China'\n",
    "dg = pd.read_excel('./Sankey_Data/'+str(year)+'_Data_for_'+str(region)+'_Sankey.xlsx', sheet_name = 'agg', index_col=0,engine='openpyxl')\n",
    "dg = dg.round(2)\n",
    "\n",
    "\n",
    "print(region)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "d0 = pd.DataFrame()\n",
    "\n",
    "for i in range(0,dg.shape[0]):\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    val_orig = dg.iloc[i,].values\n",
    "    ind = [all(tup) for tup in zip(val_orig!=0, np.isnan(val_orig)==False)]\n",
    "\n",
    "    val = val_orig[ind]\n",
    "    source = [*[dg.index[i]] * len(val)]\n",
    "    target = dg.columns[ind]\n",
    "\n",
    "    df['source']=source\n",
    "    df['target']=target\n",
    "    df['value']=val\n",
    "\n",
    "    d0=d0.append(df, ignore_index =True)\n",
    "\n",
    "d0['type']='A'\n",
    "\n",
    "#change types\n",
    "d0.at[d0[d0['source'].str.contains(\"Domestic\")].index,'type'] = 'D'\n",
    "d0.at[d0[d0['source'].str.contains(\"Foreign\")].index,'type'] = 'F'\n",
    "d0.at[d0[d0['source'].str.contains(\"Bound\")].index,'type'] = 'BI'\n",
    "d0.at[d0[d0['target'].str.contains(\"Bound\")].index,'type'] = 'BO'\n",
    "d0.at[d0[d0['source'].str.contains(\"ROW\")].index,'type'] = 'ROW'\n",
    "\n",
    "#change source names\n",
    "for i in dg.columns.values:\n",
    "    d0.at[d0[d0['source'].str.contains(str(i))].index,'source'] = str(i)\n",
    "\n",
    "df=d0\n",
    "\n",
    "#define name-pairs for the boundary outputs (BO), could also be read in from a csv\n",
    "BO=[['Mining', 'BO Gangue'],\n",
    "    ['Reduction','BO Blast furnance\\ngas & slag'],\n",
    "    ['Steelmaking','BO Steelmaking Slag']]\n",
    "\n",
    "#do the same for the boundary inputs (BI)\n",
    "BI=[['BI Crude Ore', 'Mining'],\n",
    "    ['BI Coke/air/flux','Reduction'],\n",
    "    ['BI Eol-Scrap','Scrap preparation']]\n",
    "\n",
    "#define Final Outputs strings\n",
    "FU=[['Construction','Construction'],\n",
    "    ['Manufacture of machinery and equipment n.e.c.','Machinery and \\nequipment n.e.c.'],\n",
    "   ['Manufacture of motor vehicles, trailers and semi-trailers','Motor vehicles, trailers \\nand semi-trailers'],\n",
    "    ['Manufacturing n.e.c.','Manufacturing n.e.c.'],\n",
    "   ['Manufacture of other transport equipment','Other transport \\nequipment']\n",
    "    ]\n",
    "#define further nodes which all have no next node\n",
    "ends=['Final Use']\n",
    "\n",
    "#rename Boundary Output & Inputs names for all relevant columns\n",
    "for i in range(0,len(BO)):\n",
    "    df.loc[(df['source']==BO[i][0]) & (df['target']==dg.columns[dg.shape[1]-1]),'target']=BO[i][1]\n",
    "\n",
    "for i in range(0,len(BI)):\n",
    "    df.loc[(df['target']==BI[i][1]) & (df['source']==dg.index[dg.shape[0]-1]),'source']=BI[i][0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dv_names = pd.read_excel('./Sankey_Data/'+str(year)+'_Data_for_'+str(region)+'_Sankey.xlsx', sheet_name = 'disagg_names', index_col=0,engine='openpyxl',header=None)\n",
    "dv_names=dv_names.reset_index()\n",
    "\n",
    "dv = pd.read_excel('./Sankey_Data/'+str(year)+'_Data_for_'+str(region)+'_Sankey.xlsx', sheet_name = 'disagg', index_col=0,engine='openpyxl')\n",
    "dv = dv.round(2)\n",
    "dv=dv.reset_index()\n",
    "dv.columns=['target','value']\n",
    "\n",
    "dv['source']=dg.columns[-4]\n",
    "dv['type']='A'\n",
    "\n",
    "\n",
    "#change types\n",
    "dv.at[dv[dv['target'].str.contains(\"Domestic\")].index,'type'] = 'D'\n",
    "dv.at[dv[dv['target'].str.contains(\"Foreign\")].index,'type'] = 'F'\n",
    "dv['target']=np.concatenate(np.tile(dv_names[0].values, (2, 1)))\n",
    "dv=dv[dv['value']!=0]\n",
    "\n",
    "\n",
    "db=dv\n",
    "\n",
    "\n",
    "df=df.append(db)\n",
    "# #drop Final Use rows\n",
    "df=df[df['target']!='Final Use']\n",
    "\n",
    "# df[[all(tup) for tup in zip(df['source']==dg.columns[-4], df['target'].str.contains('Scrap')==False)]]\n",
    "for i in range(0,len(FU)):\n",
    "    df.loc[df['target']==FU[i][0],'target']=FU[i][1]\n",
    "\n",
    "df=df.reset_index()\n",
    "df=df.drop('index',axis=1)\n",
    "df=df.round(2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create dataframes for the sums\n",
    "sums=[]\n",
    "for nam in np.unique(np.append(df['source'].values,df['target'].values)).tolist():\n",
    "    sums.append(df[df['source']==str(nam)]['value'].sum())\n",
    "\n",
    "sums\n",
    "dx=pd.DataFrame()\n",
    "dx['flow']=np.unique(np.append(df['source'].values,df['target'].values)).tolist()\n",
    "dx['value']= sums\n",
    "dx=dx[dx['value']!=0]\n",
    "dx=dx.round(2)\n",
    "sums1=[]\n",
    "for nam in np.unique(df['target'].values).tolist():\n",
    "    sums1.append(df[df['target']==str(nam)]['value'].sum())\n",
    "\n",
    "sums1\n",
    "dy=pd.DataFrame()\n",
    "dy['flow']=np.unique(df['target'].values).tolist()\n",
    "dy['value']= sums1\n",
    "dy=dy[dy['value']!=0]\n",
    "dy=dy.round(2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#define layers\n",
    "#first do all column which are no final node\n",
    "layers = {'layer'+str(i+1):ProcessGroup(list([dg.columns[i]]), \n",
    "            title=dg.columns[i]+'\\n'+str(dx[dx['flow']==dg.columns[i]]['value'].values[0])+'\\n'\n",
    "                            ) for i in range(0,len(dg.columns)-2)}\n",
    "\n",
    "layers1 = {'layer'+str(i+len(dg.columns)-1):ProcessGroup(list([[FU[i][1] for i in range(len(FU))][i]]),\n",
    "            title=[FU[j][1] for j in range(len(FU))][i]+'\\n'+str(dy[dy['flow']==[FU[j][1] for j in range(len(FU))][i]]['value'].values[0])+'\\n'\n",
    "            ) for i in range(0,len(np.unique(db['target'].values).tolist()))}\n",
    "\n",
    "\n",
    "#and finally the BO nodes\n",
    "BO_ind=[[v[1] for v in BO].index([ e for e in [v[1] for v in BO] if e in dy['flow'].values ][z]) for z in range(len([ e for e in [v[1] for v in BO] if e in dy['flow'].values ]))]\n",
    "layers2 = {'end'+str(i+1):ProcessGroup(list([[v[1] for v in BO][i]]), title=[v[1] for v in BO][i]+'\\n'+str(dy[dy['flow']==[v[1] for v in BO][i]]['value'].values[0])+'\\n')\n",
    "          for i in BO_ind}\n",
    "\n",
    "\n",
    "#get columns which have flows to scrap:\n",
    "scrap_flows = df[(df['target'].str.contains('Scrap')) & (df['type']!='BI')]['source'].values\n",
    "#get layer-endings & full names which have flows to scrap\n",
    "scrap_layer = list(layers.keys())[np.where(dg.columns.str.contains('Scrap'))[0][0]]\n",
    "layer_endings=[list(layers.keys())[np.where(dg.columns==scrap_flows[i])[0][0]][-1] for i in range(len(scrap_flows))]\n",
    "layer_fullnames=[list(layers.keys())[np.where(dg.columns==scrap_flows[i])[0][0]] for i in range(len(scrap_flows))]\n",
    "\n",
    "\n",
    "##############Ordering code with waypoints!!!!!:\n",
    "#first col: 1st start layer\n",
    "a=[['start1'],[],[]]\n",
    "\n",
    "#second col: first layer and other start layers\n",
    "# if scrap layer exists:...\n",
    "b=[['layer1'],['start2'],['start3']] \n",
    "\n",
    "#third col:first end layer\n",
    "c=[['end1'],['layer2',scrap_layer],[]] \n",
    "\n",
    "#define ordering list\n",
    "ordering=list()\n",
    "ordering=ordering+[a]+[b]+[c]\n",
    "#layers inbetween:\n",
    "#make empty list for waypoints:\n",
    "waypoints=list({'w1': Waypoint(direction='L', title='')}.items())+list({'ws5': Waypoint(direction='L', title='')}.items())+list({'ws6': Waypoint(direction='L', title='')}.items())\n",
    "\n",
    "\n",
    "#for i in all other layer endings except the scrap layer ending\n",
    "j=1\n",
    "for i in [x for x in [v[-1] for v in list(layers.keys())[2:]] if (x!=scrap_layer[-1])]:#&(x!=Final_use_layer[-1])]: \n",
    "    o = [[],['layer'+str(i)],[]]\n",
    "    if j<len(BO):\n",
    "        j=j+1\n",
    "        o = [['end'+str(j)],['layer'+str(i)],[]]\n",
    "        if i==[x for x in [v[-1] for v in list(layers.keys())[2:]] if (x!=scrap_layer[-1])][0]:\n",
    "            o = [['end'+str(j)],['layer'+str(i)],['w1']]\n",
    "\n",
    "        if int(i)+1 in [int(numeric_string)+1 for numeric_string in layer_endings]:  \n",
    "            o = [['end'+str(j)],['layer'+str(i)],['w'+str(int(i)+1)]]\n",
    "        waypoints=waypoints+list({'w'+str(int(i)+1): Waypoint(direction='L', title='')}.items())\n",
    "    else:\n",
    "        if int(i)+1 in [int(numeric_string)+1 for numeric_string in layer_endings]:  \n",
    "            o = [[],['layer'+str(i)],['w'+str(int(i)+1)]]\n",
    "        waypoints=waypoints+list({'w'+str(int(i)+1): Waypoint(direction='L', title='')}.items())\n",
    "    ordering=ordering+[o]\n",
    "#     print(o)\n",
    "\n",
    "#last col: Final Use\n",
    "d=[[],['layer'+str(i+len(dg.columns)-1) for i in range(0,len(np.unique(db['target'].values).tolist()))],[]]\n",
    "\n",
    "ordering=ordering+[d]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################################################   NODES:\n",
    "\n",
    "nodes = dict(list({'start'+str(i): ProcessGroup(list([[v[0] for v in BI][i-1]]), title=[v[0] for v in BI][i-1]) for i in [1,2,3]}.items())+\n",
    "     list({'end'+str(i+1): ProcessGroup(list([[v[1] for v in BO][i]]), title=[v[1] for v in BO][i]) for i in BO_ind}.items()))\n",
    "\n",
    "missing_list = list(set(['end1','end2', 'end3'])-set(list(layers2.keys())))\n",
    "missing_ends = []\n",
    "if missing_list!=[]:\n",
    "    missing_ends = list({'end'+i[3]:ProcessGroup(list('Missing')) for i in list(set(['end1','end2', 'end3'])-set(list(layers2.keys())))}.items())\n",
    "\n",
    "nodes = dict(list(nodes.items()) + list(layers.items()) + list(layers1.items()) + list(layers2.items()) + waypoints + missing_ends)\n",
    "\n",
    "#####################################################   BUNDLES:\n",
    "\n",
    "# set the \"bundle\" of connections you want to show\n",
    "import itertools\n",
    "waypointsindex = list([])\n",
    "for y in [[x for x in a][2] for a in ordering]:\n",
    "    if len(y)>0:\n",
    "        if 'w' in y[0]:\n",
    "            waypointsindex=waypointsindex+[True]\n",
    "        else: \n",
    "            waypointsindex=waypointsindex+[False]\n",
    "    else:\n",
    "        waypointsindex=waypointsindex+[False]\n",
    "\n",
    "wp_layers = [[[x for x in a][1] for a in ordering][y] for y in (np.asarray(([x for x in [np.where(waypointsindex)]][0]))[0]).tolist()][1:]\n",
    "wp_waypoints = [[[x for x in a][2] for a in ordering][y] for y in (np.asarray(([x for x in [np.where(waypointsindex)]][0]))[0]).tolist()][1:]\n",
    "\n",
    "wp_strings = list(['w1'])\n",
    "waypointstrings= list([])\n",
    "for i in range(len(wp_layers)):\n",
    "    wp_strings=[wp_waypoints[i][0]]+wp_strings\n",
    "    waypointstrings=waypointstrings+[wp_strings]\n",
    "\n",
    "bundles = [Bundle('layer'+str(i), 'layer'+str(i+1)) for i in [v+1 for v in range(len(dg.columns)-4)]]+\\\n",
    "[Bundle(scrap_layer, 'layer'+str(i)) for i in [v+1 for v in range(len(dg.columns)-3)]]+\\\n",
    "list(itertools.chain.from_iterable([[Bundle('start'+str(j+1),'layer'+str(i)) for i in [v+1 for v in range(len(dg.columns)-1)]] for j in range(len(BI))]))+\\\n",
    "list(itertools.chain.from_iterable([[Bundle('layer'+str(i),'end'+str(j+1)) for i in [v+1 for v in range(len(dg.columns)-1)]] for j in BO_ind]))+\\\n",
    "[Bundle('layer'+str(len(dg.columns)-3),'layer'+str(len(dg.columns)-2+i)) for i in range(1,len(np.unique(db['target'].values).tolist())+1)]+\\\n",
    "[Bundle(wp_layers[i][0], scrap_layer, waypointstrings[i]) for i in range(len(wp_layers))]+\\\n",
    "[Bundle(source='layer4', target='layer3', waypoints=(), flow_selection=None, flow_partition=None, default_partition=None),\n",
    " Bundle(source='layer5', target='layer3', waypoints=(), flow_selection=None, flow_partition=None, default_partition=None)]\n",
    "\n",
    "#####################################################    PLOT:\n",
    "# add the partitions\n",
    "begins = Partition.Simple('source', df['source'].unique())\n",
    "partition_type = Dataset(df).partition('type')\n",
    "\n",
    "# nodes['start'].partition = begins\n",
    "# nodes['end'].partition = Partition.Simple('target', \n",
    "#                          df['target'].unique())\n",
    "# add the color palette\n",
    "palette = {'D': '#481567',#'#FF00FF',#'#440154FF', #'#541352',\n",
    "           'F': '#FDE725', #'#FFD700',\n",
    "          'BI': '#F88379',#'#10a53d',\n",
    "          'BO': '#F5F5F5',\n",
    "          'ROW': '#00FF00'}\n",
    "\n",
    "\n",
    "# create the sankey diagram\n",
    "sdd = SankeyDefinition(nodes, bundles, ordering, \n",
    "                       flow_partition=partition_type)\n",
    "\n",
    "# display & save the sankey diagram\n",
    "weave(sdd, df, palette=palette\n",
    "     ).to_widget(width=2680, height=720, margins=dict(left=220, right=320),align_link_types=False).auto_save_png('./Sankey_Images/'+str(year)+'_'+str(region)+'.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
